## Linux 下的数据提取与合并

> 2020-02-26

### 数据状态
数据文件位置：`*/public/home/wangliqiang/program/RTAQ/obs`

大概9个月的数据，每个城市每小时一个文件，`9*30*24*316`个文件

### 数据处理目标
将各个小时文件合并为1个数据文件，包含所有城市的小时数据

### 命令
将全部一级子文件夹内的数据合并
```
cat ./*/*.csv > all.csv
```

> 问题：合并非常慢，可能是子文件和子文件太多，且合并后文件呈现以下状态，包含重复表头

```
time,cityname,devid,aqi,pm25,pm10,o3,so2,no2,co,prkey,lat,lon
2019020500,阿坝,2566A,39,27,33,56,12,13,0.5,无,31.905,102.23
2019020500,阿坝,2567A,38,25,38,51,14,5,1.4,无,31.483,103.598
2019020500,阿坝,2568A,28,8,24,87,3,1,0.3,无,31.899,102.241
time,cityname,devid,aqi,pm25,pm10,o3,so2,no2,co,prkey,lat,lon
2019020501,阿坝,2566A,42,29,34,53,16,14,0.6,无,31.905,102.23
2019020501,阿坝,2567A,38,19,38,52,12,6,1.3,无,31.483,103.598
2019020501,阿坝,2568A,28,8,24,87,3,1,0.3,无,31.899,102.241
time,cityname,devid,aqi,pm25,pm10,o3,so2,no2,co,prkey,lat,lon
2019020502,阿坝,2566A,56,37,62,55,21,10,0.4,颗粒物(PM10),31.905,102.23
```

挑选目前需要使用的数据先合并为总文件，并通过排序剔除重复行的方式去除重复表头
```
cat cat ./*/*2019-02-{05..20}* > all_raw

sort all_raw |uniq > all_sorted
```
合并后文件表头位于最后一行，其他数据按照首列排序
```
2019020500,阿坝,2566A,39,27,33,56,12,13,0.5,无,31.905,102.23
2019020500,阿坝,2567A,38,25,38,51,14,5,1.4,无,31.483,103.598
2019020500,阿坝,2568A,28,8,24,87,3,1,0.3,无,31.899,102.241
2019020501,阿坝,2566A,42,29,34,53,16,14,0.6,无,31.905,102.23
2019020501,阿坝,2567A,38,19,38,52,12,6,1.3,无,31.483,103.598
2019020501,阿坝,2568A,28,8,24,87,3,1,0.3,无,31.899,102.241
2019020502,阿坝,2566A,56,37,62,55,21,10,0.4,颗粒物(PM10),31.905,102.23
2019020502,阿坝,2567A,31,18,31,46,49,6,0.8,无,31.483,103.598
2019020502,阿坝,2568A,28,8,24,87,3,1,0.3,无,31.899,102.241
2019020503,阿坝,2566A,92,68,81,54,24,10,0.4,细颗粒物(PM2.5),31.905,102.23
```